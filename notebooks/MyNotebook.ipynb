{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "streaming-floating",
   "metadata": {},
   "source": [
    "Hello world from (Scala) Spark!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "golden-wales",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2244be9b07034baa81831bbdd32b49f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1616422888890_0003</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-20-139.ec2.internal:20888/proxy/application_1616422888890_0003/\" class=\"emr-proxy-link\" emr-resource=\"j-EUO6QT8VQRA1\n",
       "\" application-id=\"application_1616422888890_0003\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-21-14.ec2.internal:8042/node/containerlogs/container_1616422888890_0003_01_000001/livy\" >Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world!\n"
     ]
    }
   ],
   "source": [
    "println(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "built-programming",
   "metadata": {},
   "source": [
    "A simple word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "violent-friday",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f6a96787bdb4b33b8525432de0386ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: String = tre tigri contro tre tigri\n",
      "res: Array[(String, Int)] = Array((tigri,2), (tre,2), (contro,1))\n"
     ]
    }
   ],
   "source": [
    "val sentence: String = \"tre tigri contro tre tigri\"\n",
    "val res: Array[(String, Int)] = sc.parallelize(sentence.split(\" \")).map((_, 1)).reduceByKey(_ + _).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-celtic",
   "metadata": {},
   "source": [
    "Save the result to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dress-conversion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75b65af0767c4dabba5a12209f8f5939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.hadoop.fs.{FileSystem, Path}\n",
      "fs: org.apache.hadoop.fs.FileSystem = DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_1600703682_22, ugi=livy (auth:SIMPLE)]]\n",
      "outPutPath: org.apache.hadoop.fs.Path = wordcount\n",
      "res28: AnyVal = true\n",
      "hdfspath: String = wordcount\n",
      "writeandread: (path: String)Array[String]\n",
      "res32: Array[String] = Array((tigri,2), (tre,2), (contro,1))\n"
     ]
    }
   ],
   "source": [
    "import org.apache.hadoop.fs.{FileSystem, Path}\n",
    "val fs = FileSystem.get(sc.hadoopConfiguration) // get the file system\n",
    "val outPutPath = new Path(path)\n",
    "if (fs.exists(outPutPath)) { // delete the HDFS folder if exists\n",
    "    fs.delete(outPutPath, true)\n",
    "}\n",
    "\n",
    "val hdfspath: String = \"wordcount\" // HDFS path\n",
    "def writeandread(path: String) = {\n",
    "    sc.parallelize(res).saveAsTextFile(path) // save the RDD\n",
    "    val rdd = sc.textFile(path) // read it back\n",
    "    rdd.collect() // print it \n",
    "}\n",
    "\n",
    "writeandread(hdfspath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weird-fence",
   "metadata": {},
   "source": [
    "... and to S3 as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "advance-heather",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0de75c8ccc647a4b9f0d84db403a283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory s3://aws-emr-resources-604905954159-us-east-1/wordcount already exists\n",
      "  at org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n",
      "  at org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:289)\n",
      "  at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n",
      "  at org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\n",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n",
      "  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\n",
      "  at org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\n",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n",
      "  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n",
      "  at org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)\n",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n",
      "  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)\n",
      "  at org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)\n",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n",
      "  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\n",
      "  at org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1552)\n",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n",
      "  at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1552)\n",
      "  at org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1538)\n",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n",
      "  at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1538)\n",
      "  at writeandread(<console>:36)\n",
      "  ... 53 elided\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val s3bucket: String = \"aws-emr-resources-604905954159-us-east-1\"\n",
    "val s3path: String = s\"s3://${s3bucket}/wordcount\"\n",
    "writeandread(s3path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charitable-investigator",
   "metadata": {},
   "source": [
    "[Exercise] Upload a file using Amazon SDK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "champion-venice",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08385a1861ac4557b85e8d567c764f93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key: String = divinacommedia/inferno.txt\n"
     ]
    }
   ],
   "source": [
    "val key: String = \"divinacommedia/inferno.txt\"\n",
    "try {\n",
    "    val s3Client = AmazonS3ClientBuilder.standard().build()\n",
    "    // s3Client.deleteObject(bucketName, key)\n",
    "    s3Client.deleteObject(bucketName, ...) // Fill in the gaps, program it to upload the file content\n",
    "} catch {\n",
    "    case e: Throwable => e.printStackTrace()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "built-mineral",
   "metadata": {},
   "source": [
    "From Spark to Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "brave-fault",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "169096c614f0423793158686df7995dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.sql.hive.HiveContext\n",
      "warning: there was one deprecation warning (since 2.0.0); for details, enable `:setting -deprecation' or `:replay -deprecation'\n",
      "hiveContext: org.apache.spark.sql.hive.HiveContext = org.apache.spark.sql.hive.HiveContext@7eb10552\n",
      "import hiveContext.implicits._\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.hive.HiveContext\n",
    "val hiveContext = new HiveContext(sc)\n",
    "import hiveContext.implicits._\n",
    "\n",
    "sc.textFile(s\"s3://${s3bucket}/inferno.txt\").flatMap(_.split(\" \")).filter(_.nonEmpty).map((_, 1)).reduceByKey(_ + _).toDF(\"word\", \"count\").write.mode(\"overwrite\").saveAsTable(\"inferno\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "controversial-sharp",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59e65686789e45c2a761a20fd655fe9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "|   e| 3613|\n",
      "| che| 3535|\n",
      "|  la| 2261|\n",
      "|  di| 1823|\n",
      "|   a| 1805|\n",
      "| non| 1338|\n",
      "| per| 1319|\n",
      "|  in| 1071|\n",
      "|  si| 1042|\n",
      "|  ?l|  958|\n",
      "|  le|  777|\n",
      "|  li|  753|\n",
      "|  mi|  736|\n",
      "|  s?|  706|\n",
      "|  il|  652|\n",
      "| pi?|  637|\n",
      "| con|  623|\n",
      "|   ?|  600|\n",
      "|  de|  593|\n",
      "|  da|  593|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql(\"select word, count from inferno order by 2 desc\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

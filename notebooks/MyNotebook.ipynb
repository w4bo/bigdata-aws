{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "streaming-floating",
   "metadata": {},
   "source": [
    "Hello world from (Scala) Spark!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-wales",
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "built-programming",
   "metadata": {},
   "source": [
    "A simple word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "violent-friday",
   "metadata": {},
   "outputs": [],
   "source": [
    "val sentence: String = \"tre tigri contro tre tigri\"\n",
    "val res: Array[(String, Int)] = sc.parallelize(sentence.split(\" \")).map((_, 1)).reduceByKey(_ + _).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-celtic",
   "metadata": {},
   "source": [
    "Save the result to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dress-conversion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.hadoop.fs.{FileSystem, Path}\n",
    "val fs = FileSystem.get(sc.hadoopConfiguration) // get the file system\n",
    "val hdfspath: String = \"wordcount\" // HDFS path\n",
    "\n",
    "val outPutPath = new Path(hdfspath)\n",
    "if (fs.exists(outPutPath)) { // delete the HDFS folder if exists\n",
    "    fs.delete(outPutPath, true)\n",
    "}\n",
    "\n",
    "def writeandread(path: String) = {\n",
    "    sc.parallelize(res).saveAsTextFile(path) // save the RDD\n",
    "    val rdd = sc.textFile(path) // read it back\n",
    "    rdd.collect() // print it \n",
    "}\n",
    "\n",
    "writeandread(hdfspath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weird-fence",
   "metadata": {},
   "source": [
    "... and to S3 as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advance-heather",
   "metadata": {},
   "outputs": [],
   "source": [
    "val s3bucket: String = \"aws-emr-resources-604905954159-us-east-1\"\n",
    "val s3path: String = s\"s3://${s3bucket}/wordcount\"\n",
    "writeandread(s3path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "built-mineral",
   "metadata": {},
   "source": [
    "From Spark to Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-fault",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.hive.HiveContext\n",
    "val hiveContext = new HiveContext(sc)\n",
    "import hiveContext.implicits._\n",
    "\n",
    "sc.textFile(s\"s3://${s3bucket}/inferno.txt\").flatMap(_.split(\" \")).filter(_.nonEmpty).map((_, 1)).reduceByKey(_ + _).toDF(\"word\", \"count\").write.mode(\"overwrite\").saveAsTable(\"inferno\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-sharp",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql(\"select word, count from inferno order by 2 desc\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
